1. Write Python code to validate a JSON structure and flatten nested fields.

2. How to handle schema drift while ingesting from REST APIs?

3. Write Python script to read and process a csv file with dynamic schema

4. Write SQL to identify churned users over last 90 days
You are given a table user_activity with the following columns:

Column Name	| Data Type |	Description
user_id	| INT |	Unique identifier for the user
activity_date |	DATE |	Date of the userâ€™s activity
status | STRING	| Status of the user: 'active' or 'inactive'

A churned user is defined as a user who:
* Was active at least once in the past, before the last 90 days.
* Has no activity in the last 90 days from today.

Write an SQL query to find the list of churned users, along with the date of their last activity. 
5. Optimize the previous question for large tables (hint: use partitioning and indexing).

6. How to handle late-arriving data and reprocessing in ADF? 

7. Join 2 dataframes with different schema using row number


Jio Data Engineering interviews will be 17X easier with these questions
CTC = 26 LPA

1. How would you design a real-time data pipeline to ingest and process telecom usage data?
2. Write a SQL query to get the second highest recharge amount per user in the last 90 days.
3. What is the difference between narrow and wide transformations in Apache Spark?
4. How do you optimize a Spark join when one of the datasets is heavily skewed?
5. What happens if Zookeeper fails in a Kafka cluster?
6. How would you implement Slowly Changing Dimension Type 2 (SCD2) using PySpark?
7. How would you partition and store billions of records in Hive for fast query performance?
8. What are some data quality checks you would implement in an ETL pipeline?
9. How does Spark ensure fault tolerance using DAGs?
10. What is the difference between repartition() and coalesce() in Spark, and when would you use them?
11. How would you design a schema to store mobile app usage logs for analytics purposes?
12. How do you handle late-arriving events in a structured streaming pipeline?
13. Compare Kafka and Flume. Which would you use for real-time data ingestion and why?
14. How do you manage schema evolution in daily-ingested Parquet files?
15. What steps would you take to debug and fix a PySpark job that runs out of memory?

ğŸ®+ ğ—˜ğ˜…ğ—½ğ—²ğ—¿ğ—¶ğ—²ğ—»ğ—°ğ—²ğ—± ğ—Ÿğ—²ğ˜ƒğ—²ğ—¹ ğ——ğ—®ğ˜ğ—® ğ—˜ğ—»ğ—´ğ—¶ğ—»ğ—²ğ—²ğ—¿ğ—¶ğ—»ğ—´ ğ—¾ğ˜‚ğ—²ğ˜€ğ˜ğ—¶ğ—¼ğ—»ğ˜€.

1. Explain how Spark handles data in memory vs on disk.
2. Whatâ€™s the difference between repartition and coalesce in PySpark?
3. How would you design a data pipeline to handle daily logs from multiple sources?
4. How do you handle schema evolution in a Parquet file?
5. Explain different types of joins in PySpark with examples.
6. How do you optimize a slow-running Spark job?
7. Whatâ€™s the difference between narrow and wide transformations in Spark?
8. What is watermarking in streaming data processing?
9. How do you handle late-arriving data in a batch pipeline?
10. Explain a situation where you had to clean and transform messy JSON data.

ğŸ±+ ğ—˜ğ˜…ğ—½ğ—²ğ—¿ğ—¶ğ—²ğ—»ğ—°ğ—²ğ—± ğ—Ÿğ—²ğ˜ƒğ—²ğ—¹ ğ——ğ—®ğ˜ğ—® ğ—˜ğ—»ğ—´ğ—¶ğ—»ğ—²ğ—²ğ—¿ğ—¶ğ—»ğ—´ ğ—¾ğ˜‚ğ—²ğ˜€ğ˜ğ—¶ğ—¼ğ—»ğ˜€.

1. Design a scalable architecture to process real-time clickstream data.
2. How do you handle data consistency in distributed systems?
3. How would you build a fault-tolerant data ingestion system using Kafka?
4. Whatâ€™s your strategy to backfill historical data without affecting the current pipeline?
5. Explain your approach to designing an end-to-end data platform from scratch.
6. How do you manage data governance, lineage, and auditing in pipelines?
7. Describe a time you optimized a data pipeline and reduced cost/performance significantly.
8. How would you build a data lakehouse architecture using Delta Lake or Apache Hudi?
9. How do you monitor data quality in a highly dynamic pipeline?
10. What are the best practices for partitioning huge datasets for analytical queries?

Data Warehousing interview questions 

ğ—£ğ—›ğ—”ğ—¦ğ—˜ ğŸ­ - ğ—•ğ—®ğ˜€ğ—¶ğ—°ğ˜€ 

- What are the main differences between a database and a data warehouse?
- Why do organizations need a separate system for analytical processing?
- What is data granularity in a data warehouse?
- Can you explain what a data mart is and how it differs from a data warehouse?
- What is metadata in data warehousing, and what role does it play?

ğ—£ğ—›ğ—”ğ—¦ğ—˜ ğŸ® - ğ—œğ—»ğ˜ğ—²ğ—¿ğ—ºğ—²ğ—±ğ—¶ğ—®ğ˜ğ—²

- What are conformed dimensions, and why are they important in data warehouse design?
- How would you manage historical data in a data warehouse?
- What is the difference between full load and incremental load in ETL?
- Explain the concept of late arriving dimensions and how to handle them.
- How do you decide between star schema and snowflake schema in a project?

ğ—£ğ—›ğ—”ğ—¦ğ—˜ ğŸ¯ - ğ—”ğ—±ğ˜ƒğ—®ğ—»ğ—°ğ—²ğ—±

- How would you implement data lineage tracking in a data warehouse system?
- What strategies would you use to scale a data warehouse as data volume grows rapidly?
- How do you manage schema evolution in a data warehouse without disrupting existing reports?
- What are the challenges of integrating semi-structured or unstructured data into a data warehouse?
- How do cloud-based data warehouses (like Snowflake or Synapse) differ from traditional on-premise ones?

ğŸ®ğ—»ğ—± ğ—¥ğ—¼ğ˜‚ğ—»ğ—± (ğ—©ğ—¶ğ—¿ğ˜ğ˜‚ğ—®ğ—¹ ğ—¢ğ—»-ğ—¦ğ—¶ğ˜ğ—² ğ——ğ—®ğ˜ğ—® ğ—Ÿğ—¼ğ—¼ğ—½ - ğŸ° ğ—¥ğ—¼ğ˜‚ğ—»ğ—±ğ˜€)

ğ—¥ğ—¼ğ˜‚ğ—»ğ—± ğŸ­ - ğ—£ğ˜†ğ˜ğ—µğ—¼ğ—» + ğ—¦ğ—½ğ—®ğ—¿ğ—¸ ğ—–ğ—¼ğ—±ğ—¶ğ—»ğ—´ (ğ—¥ğ—²ğ—®ğ—¹-ğ—ªğ—¼ğ—¿ğ—¹ğ—± ğ—¦ğ—°ğ—²ğ—»ğ—®ğ—¿ğ—¶ğ—¼ ğ—•ğ—®ğ˜€ğ—²ğ—±):
 ã† Transform raw booking data using PySpark (e.g., calculate cancellations per region)
 ã† Use of groupBy(), withColumn(), window functions, repartitioning, etc.
 ã† Discussion around wide vs narrow transformations and when to use caching

ğ—¥ğ—¼ğ˜‚ğ—»ğ—± ğŸ® - ğ——ğ—®ğ˜ğ—® ğ— ğ—¼ğ—±ğ—²ğ—¹ğ—¶ğ—»ğ—´ & ğ—”ğ—¿ğ—°ğ—µğ—¶ğ˜ğ—²ğ—°ğ˜ğ˜‚ğ—¿ğ—²:
 ã† Design schema for Airbnb review system (handle normalization, indexing)
 ã† Create scalable data model for analytics use cases (e.g., review trends over time)
 ã† Discuss denormalization, partitioning, surrogate keys, SCD types

ğ—¥ğ—¼ğ˜‚ğ—»ğ—± ğŸ¯ - ğ—˜ğ—§ğ—Ÿ ğ——ğ—²ğ˜€ğ—¶ğ—´ğ—» + ğ—¦ğ˜†ğ˜€ğ˜ğ—²ğ—º ğ—§ğ—µğ—¶ğ—»ğ—¸ğ—¶ğ—»ğ—´:
 ã† Architect a full ETL pipeline from ingestion to transformation and load
 ã† Include data validation, logging, retries, and monitoring
 ã† Tools: Airflow, Spark, Redshift, S3, or similar
 ã† Discuss trade-offs: Batch vs streaming, orchestration logic, schema evolution

ğ—¥ğ—¼ğ˜‚ğ—»ğ—± ğŸ° - ğ—•ğ—²ğ—µğ—®ğ˜ƒğ—¶ğ—¼ğ—¿ğ—®ğ—¹ + ğ—–ğ—¿ğ—¼ğ˜€ğ˜€-ğ—³ğ˜‚ğ—»ğ—°ğ˜ğ—¶ğ—¼ğ—»ğ—®ğ—¹ ğ—–ğ—¼ğ—¹ğ—¹ğ—®ğ—¯ğ—¼ğ—¿ğ—®ğ˜ğ—¶ğ—¼ğ—»:
 ã† STAR format answers to past experiences (conflicts, ownership, stakeholder management)
 ã† How you work with data scientists, PMs, and analytics teams

 1. You are asked to design a data pipeline to track user impressions and clicks in real-time. How would you design it to ensure scalability and low latency?
2. During daily batch processing, your pipeline fails halfway through due to a corrupted file. How would you detect and recover from such failures automatically?
3. Suppose you receive data from multiple ad exchanges in different formats (CSV, JSON, Avro). How would you design a unified ingestion and transformation process?
4. Your Spark job is running slower than expected. It processes 2 TB of data, but the job takes 3x more time compared to last week. How would you debug and fix the issue?
5. A real-time Kafka stream is receiving duplicate ad events due to retries at the source. How would you handle deduplication in your streaming pipeline using Spark Structured Streaming?
6. Management asks for aggregated campaign performance data every 15 minutes for dashboards. How would you build a system that supports this near real-time requirement?
7. You need to join two very large datasets (30 TB+) to create a unified view. What join strategy would you use in Spark and how would you optimize it?
8. You're seeing frequent memory errors in your PySpark cluster during transformations. What configuration parameters and tuning steps would you apply to avoid such crashes?
9. Youâ€™ve noticed an increase in late-arriving events in your streaming system. How would you handle them to maintain correctness in time-based aggregations?
10. A product manager wants a reliable daily report of revenue per publisher, but there are frequent mismatches in numbers. How would you ensure data accuracy in the pipeline?

- Python: Write a function to detect schema anomalies in logs

ğŸ­ğ˜€ğ˜ ğ—¥ğ—¼ğ˜‚ğ—»ğ—± - ğ——ğ—®ğ˜ğ—® ğ—˜ğ—»ğ—´ğ—¶ğ—»ğ—²ğ—²ğ—¿ğ—¶ğ—»ğ—´ ğ—¥ğ—¼ğ˜‚ğ—»ğ—±
- Design ETL pipeline: ingesting data from external API into BigQuery
- Questions on schema evolution handling, Airflow DAG frequency (daily vs trigger)
- SQL query optimization challenge

ğŸ®ğ—»ğ—± ğ—¥ğ—¼ğ˜‚ğ—»ğ—± - ğ—–ğ—¼ğ—±ğ—¶ğ—»ğ—´ ğ—¥ğ—¼ğ˜‚ğ—»ğ—±
- Given a GB-scale log file, extract top 10 users by event frequency
- Must support memory optimization + streaming
- Follow-up: Handle schema change in streaming data

ğŸ¯ğ—¿ğ—± ğ—¥ğ—¼ğ˜‚ğ—»ğ—± - ğ—¦ğ˜†ğ˜€ğ˜ğ—²ğ—º ğ——ğ—²ğ˜€ğ—¶ğ—´ğ—»
- Design a real-time clickstream pipeline
- Discuss fault tolerance, deduplication logic, and storing 1B rows efficiently
- Stack: Pub/Sub, Dataflow, BigQuery, Z-Ordering, Spark

ğŸ°ğ˜ğ—µ ğ—¥ğ—¼ğ˜‚ğ—»ğ—± - ğ—•ğ—²ğ—µğ—®ğ˜ƒğ—¶ğ—¼ğ—¿ğ—®ğ—¹ (ğ—šğ—¼ğ—¼ğ—´ğ—¹ğ—¶ğ—»ğ—²ğ˜€ğ˜€)
- Took ownership of failing project, STAR format
- Conflict with PM and missed deadline scenarios
- Showed humility and growth mindset


1. Write an SQL query to find the second highest salary from an employee table.

2. How do you handle NULL values in SQL joins?

3. Write a Python script to read a CSV file and load it into a DataFrame.

4. How do you handle exceptions in Python using try-except blocks?

5. In PySpark, how would you perform a join operation between two large DataFrames efficiently?

6. Write a PySpark code to find the top 3 customers with the highest revenue per region.

7. What is the difference between partitioning and bucketing in PySpark?

8. How do you implement Slowly Changing Dimensions (SCD) in a data warehouse?

9. Explain the concept of star schema and snowflake schema in data modeling.

10. How would you design a fact table for an e-commerce platform?

11. How do you build an ETL pipeline using Azure Data Factory?

12. What are the different types of triggers in ADF and when to use them?

13. Explain the architecture of Azure Databricks and its integration with Delta Lake.

14. Write a PySpark code to process streaming data from Event Hub in Databricks.

15. How do you optimize query performance in Azure Synapse Analytics?

16. How would you design a data warehouse for a retail business using Synapse?

17. What are the best practices for securing data in Azure Data Lake Storage?

18. How do you manage access control and secrets using Azure Key Vault?

19. Write a PySpark script to load data from ADLS into a Delta table.

20. How do you implement data lineage and governance in Microsoft Purview?

21. Build a real-time analytics pipeline using Event Hub, Stream Analytics, and Synapse.

22. How would you handle late-arriving data in a batch ETL pipeline?

23. Write an SQL query to calculate the customer churn rate over the last 6 months.

24. How do you implement incremental data loading in ADF pipelines?

25. Write a Python script to validate data quality and detect anomalies.

26. How do you perform schema evolution in Delta Lake?

27. How would you design a data pipeline to handle both batch and streaming data?

28. Write a PySpark code to perform window functions for ranking sales data.

29. How do you optimize storage and query performance in a Synapse dedicated pool?

30. Build an end-to-end data pipeline that ingests data from multiple sources, transforms it in Databricks, and loads it into Synapse for reporting.

My friend got a 30+ LPA Job Offer from Morgan Stanley
Position: Data Engineer
Application Method: Referral.

ğ—£ğ—¿ğ—²ğ—¹ğ—¶ğ—ºğ—¶ğ—»ğ—®ğ—¿ğ˜† ğ—¥ğ—¼ğ˜‚ğ—»ğ—± (ğ—¢ğ—»ğ—¹ğ—¶ğ—»ğ—² ğ—§ğ—²ğ˜€ğ˜)
 ã† SQL Coding (Window functions, joins, subqueries)
 ã† Python & SQL MCQs (medium to hard)
 ã† Data Structure Coding (arrays, strings â€“ medium level)
 ã† DBMS, Unix, and OS-based MCQs

ğŸ­ğ˜€ğ˜ ğ—¥ğ—¼ğ˜‚ğ—»ğ—± (ğ—§ğ—²ğ—°ğ—µğ—»ğ—¶ğ—°ğ—®ğ—¹ ğ—œğ—»ğ˜ğ—²ğ—¿ğ˜ƒğ—¶ğ—²ğ˜„ ğŸ­)
SQL
 ã† Given Tables A & B with column â€˜idâ€™ â†’ Explain output count for inner, left, right, and full outer joins
 ã† Find employee with highest salary per department using DENSE_RANK()
 ã† Explain why DENSE_RANK() was used instead of RANK()
 ã† Discuss use cases of LEAD, LAG, and NTILE window functions
Hive & Sqoop:
 ã† Difference between Managed vs External tables, Hive partitioning vs bucketing
Big Data Concepts:
 ã† HDFS Rack Awareness, Spark Job Submission Flow
 ã† Spark: Narrow vs Wide Transformations, coalesce() vs repartition()
 ã† Scheduling Spark Jobs in Databricks
Cloud Computing:
 ã† AWS EC2, IAM Roles/Policies, S3 Storage
 ã† Code to upload CSV to S3 using Boto3 (pseudo-code accepted)

ğŸ®ğ—»ğ—± ğ—¥ğ—¼ğ˜‚ğ—»ğ—± (ğ—§ğ—²ğ—°ğ—µğ—»ğ—¶ğ—°ğ—®ğ—¹ ğ—œğ—»ğ˜ğ—²ğ—¿ğ˜ƒğ—¶ğ—²ğ˜„ ğŸ® â€“ ğ—©ğ—¶ğ—°ğ—² ğ—£ğ—¿ğ—²ğ˜€ğ—¶ğ—±ğ—²ğ—»ğ˜)
Data Modeling:
 ã† Created normalized and denormalized models for a relational schema
Databricks Lakehouse Architecture:
 ã† Detailed explanation of ingestion, transformation flow
 ã† Used AWS Glue + Redshift for ETL pipeline explanation
Scenario-Based ETL Design:
 ã† Preliminary checks while building an ETL
 ã† Role of staging layer in pipeline
Batch vs Stream Processing:
 ã† Spark structured streaming concepts
Unit Testing:
 ã† How to create unit tests for SQL and PySpark code
Pseudo Code:
 ã† Build ETL pipeline using Python and Pandas to clean, transform, and save CSV
Spark Monitoring & Optimization:
 ã† Explained tuning techniques and monitoring tools used

ğ—§ğ—²ğ—°ğ—µğ—»ğ—¼ ğ— ğ—®ğ—»ğ—®ğ—´ğ—²ğ—¿ğ—¶ğ—®ğ—¹ ğ—¥ğ—¼ğ˜‚ğ—»ğ—±
Resume walkthrough: Projects, internship at ZS Associates, research papers 
Leadership & Situational Questions:
 ã† Handling team challenges, ownership, and leadership experience
 ã† Alignment with Morgan Stanley core principles
Technical Discussions:
 ã† Delta Lake and Parquet format advantages
 ã† Delta Table use cases
 ã† Spark Cluster configuration â€“ memory, cores, workers, executors
 ã† AWS Glue fetching metadata from S3 (CSV/Parquet)
 ã† Redshift and relational database experience
 ã† Internal workings of Hadoop

ğ—›ğ—¥ ğ—¥ğ—¼ğ˜‚ğ—»ğ—±
 ã† Big Data project overview
 ã† Strengths, weaknesses, hobbies
 ã† Career goals and motivation for joining Morgan Stanley
 ã† Salary discussion and confirmation

Citi Bank Data Engineering interviews will be 17X easier with these questions
CTC = 29 LPA

1. Explain your experience migrating data from Teradata (or another data warehouse) to Hadoop or a cloud platform.
2. How have you implemented Slowly Changing Dimensions Type 2 in a big data pipeline?
3. Describe how you detected and resolved data skew in your Spark jobs.
4. Compare Sparkâ€™s repartition vs coalesce and when to use each.
5. Whatâ€™s the difference between bucketing and partitioning tables? When would you use each?
6. How do you debug a Spark job that suddenly began running much slower?
7. Explain your process for automating data pipelines using tools like Oozie, Airflow, or Azure Data Factory.
8. What role does Amazon Deequ (or similar data quality frameworks) play in your data pipelines?
9. Design a pipeline to process millions of records per minute in PySpark. What architecture would you use?
10. Describe how you implemented window functions in Spark or SQL for time-series data.
11. How do you prevent duplicate records in Spark or streaming jobs?
12. Given a 1â€¯TB file in HDFS, how would you run a word count or simple transformation?
13. Walk me through a problem where you had to optimize a slow-running ETL job. What techniques did you apply?
14. How do you integrate CI/CD for data pipelines, especially for Spark or Azure Data Factory workflows?
15. Describe how youâ€™d design a scalable and fault-tolerant data pipeline on Azure (using Synapse, Blob, ADF, etc.).

Coforge keeps asking these Data Engineering questions repeatedly.
CTC - 25 LPA
EXP - 4+

1. How would you find duplicate records in a table and delete only the duplicates while keeping one copy?
2. A table has a start_date and end_date column. How would you calculate the total number of overlapping days across all rows?
3. How would you read a large CSV file in chunks, filter only required rows, and write the output into a new file using Python?
4. You have a nested dictionary of JSON records from an API. How would you flatten it and load it into a Pandas DataFrame?
5. How would you handle null values and incorrect casing in a PySpark DataFrame?
6. Write a PySpark logic to compute the difference in days between two status changes for each user.
7. How would you write a PySpark job to merge (upsert) data from a new file into an existing Delta table?
8. How can you dynamically copy multiple tables from an on-prem SQL Server to Azure Data Lake using ADF?
9. You need to delete files from multiple containers daily before writing new data. ADF doesnâ€™t support wildcards across containers in one delete activity. How would you design this?
10. Explain how you would build a Medallion Architecture in Databricks to process daily sales data.
11. You need to track policy status changes over time. How would you use PySpark and Delta Lake to implement this in Databricks?
12. What are the key differences between Dedicated SQL Pool and Serverless SQL Pool in Synapse?
13. You are asked to ingest and transform 100GB of CSV files daily in Synapse. What is your approach?
14. Difference bw reducebykey and groupbykey and when to use what?

Recently, I got a 26 LPA Job Offer from Deloitte
Position: Data Engineer
Application Method: Got a call from Naukri.com

ğ—£ğ—µğ—¼ğ—»ğ—² ğ—¦ğ—°ğ—¿ğ—²ğ—²ğ—»ğ—¶ğ—»ğ—´ ğ—¥ğ—¼ğ˜‚ğ—»ğ—±
- General discussion on profile and past experience
- No theoretical questions, directly moved to hands-on SQL questions

ğŸ­ğ˜€ğ˜ ğ—¥ğ—¼ğ˜‚ğ—»ğ—± (ğ—§ğ—²ğ—°ğ—µğ—»ğ—¶ğ—°ğ—®ğ—¹ ğ—–ğ—¼ğ—±ğ—¶ğ—»ğ—´)
- Medium-level SQL query: Calculate total revenue per customer using Orders and Order_Items tables
- Write SQL to get customers who spent the most in the last month (based on order_date)
- PySpark scenario: Join two DataFrames with different schema using row numbers
- Topics discussed:
ã† Writing CTEs and using aggregate functions
ã†Date filtering using functions like DATE_SUB
ã† PySpark Window functions for row_number and joining logic

ğŸ®ğ—»ğ—± ğ—¥ğ—¼ğ˜‚ğ—»ğ—± (ğ—§ğ—²ğ—°ğ—µ ğ—Ÿğ—²ğ—®ğ—± ğ——ğ—¶ğ˜€ğ—°ğ˜‚ğ˜€ğ˜€ğ—¶ğ—¼ğ—»)
- In-depth discussion on past project architecture
- What technologies were used and how much data was handled
- Challenges faced in Spark-based pipelines and how they were resolved
- PySpark live coding:
ã† Write code to find customers who spent more than â‚¹1000 from store S1 in the last month
ã† Required working with three datasets: Customer, Store, and Sales
ã† Logic included DataFrame joins, date filtering using add_months(), and aggregation

ğ— ğ—®ğ—»ğ—®ğ—´ğ—²ğ—¿ğ—¶ğ—®ğ—¹ ğ—¥ğ—¼ğ˜‚ğ—»ğ—±
- Focused on stakeholder and leadership discussions
- Real-world project challenges and team collaboration
- Discussion on how business requirements were translated into scalable pipelines

ğ—¦ğ—¤ğ—Ÿ:
â€¢ Use of Window Functions
â€¢ Recursive Query usage
â€¢ Identify numbers appearing three times consecutively in a table

ğ—£ğ˜†ğ˜ğ—µğ—¼ğ—»:
â€¢ String/List/Dict-based coding problems
â€¢ Theory-based questions on:
â€¢ Python Decorators
â€¢ Multiprocessing vs Multithreading (basic understanding)

ğŸ®ğ—»ğ—± ğ—¥ğ—¼ğ˜‚ğ—»ğ—± - ğ—ŸğŸ® ğ—§ğ—²ğ—°ğ—µğ—»ğ—¶ğ—°ğ—®ğ—¹ ğ—¥ğ—¼ğ˜‚ğ—»ğ—±

ğ—¦ğ—½ğ—®ğ—¿ğ—¸ ğ—–ğ—¼ğ—»ğ—°ğ—²ğ—½ğ˜ğ˜€:
â€¢ Broadcast Join vs Shuffle Join
â€¢ Salting to handle skewed data
â€¢ Bloom Filters
â€¢ Spark Memory Management: heap, garbage collection
â€¢ Repartition vs Coalesce (use cases)
â€¢ Difference between SparkContext and SparkSession
â€¢ PySpark + Snowflake integration questions

ğ—£ğ˜†ğ—¦ğ—½ğ—®ğ—¿ğ—¸ ğ—–ğ—¼ğ—±ğ—¶ğ—»ğ—´:
â€¢ DataFrame API-based joins and window functions
â€¢ Had to write logic manually in Notepad (no IDE support)

ğŸ¯ğ—¿ğ—± ğ—¥ğ—¼ğ˜‚ğ—»ğ—± â€“ ğ—§ğ—²ğ—°ğ—µğ—»ğ—¼-ğ— ğ—®ğ—»ğ—®ğ—´ğ—²ğ—¿ğ—¶ğ—®ğ—¹ ğ—¥ğ—¼ğ˜‚ğ—»ğ—±

ğ—£ğ—¿ğ—¼ğ—·ğ—²ğ—°ğ˜ & ğ—”ğ—¿ğ—°ğ—µğ—¶ğ˜ğ—²ğ—°ğ˜ğ˜‚ğ—¿ğ—²:
â€¢ Discussion on end-to-end data pipeline using:
Azure + Databricks + Airflow + Snowflake + Spark
â€¢ Asked to draw full data flow using draw.io

ğ—¦ğ—°ğ—²ğ—»ğ—®ğ—¿ğ—¶ğ—¼-ğ—•ğ—®ğ˜€ğ—²ğ—± ğ—¤ğ˜‚ğ—²ğ˜€ğ˜ğ—¶ğ—¼ğ—»ğ˜€:
â€¢ Cost optimization on Spark clusters
â€¢ Infra-level vs data-level issue resolution
â€¢ Handling critical delivery during resource unavailability
â€¢ Managing cross-team dependencies

Genpact Data Engineering interview questions 2025
CTC = 27 LPA

1. Write an SQL query to identify power users who created more than 1,000 support tickets in a specific month (e.g., June 2022).
.2. Compute the average monthly rating per product using SQL aggregation over review data.
3. Explain the difference between EXCEPT (or MINUS) and JOIN in SQL, and give examples.
4. Calculate average handling time per agent (in minutes or hours) using timestamp difference functions.
5. Describe RANK() vs. DENSE_RANK() SQL window functions and when to choose each.
6. Explain an end-to-end ETL pipeline you've built or designed, including tools and data formats.
7. Optimize a PySpark jobâ€”what steps (partitioning, caching, file formats) would you apply?
8. Design a robust data pipeline for real-time vs. batch data ingestionâ€”what would change?
9. Which AWS/Azure services have you used for data engineering, and how did you leverage them?
10. How would you build a data lake on cloud storage and integrate schema management and governance tools?
11. Describe a challenging data problem from your project and your solution.
12. Discuss your experience with Spark optimizer (Catalyst) and how it influences query planning.
13. Explain your experience with containerization: Docker/Kubernetes for reproducible data pipelines.
14. Describe SQL vs. NoSQL storage models and when you've opted for one over the other.
15. Explain logical vs. physical data independence, and why it matters in database schema design.

I screwed up 3 interviews because I couldn't explain my projects properly.

Here is how you can explain your project in an interview.

When youâ€™re in an interview, itâ€™s important to know how to talk about your projects in a way that impresses the interviewer. Here are some key points to help you do just that:

â¤ ğ—£ğ—¿ğ—¼ğ—·ğ—²ğ—°ğ˜ ğ—¢ğ˜ƒğ—²ğ—¿ğ˜ƒğ—¶ğ—²ğ˜„: Give a quick 30-second summary. Whatâ€™s the project about?

â¤ ğ—£ğ—¿ğ—¼ğ—¯ğ—¹ğ—²ğ—º ğ—¦ğ˜ğ—®ğ˜ğ—²ğ—ºğ—²ğ—»ğ˜: What problem did you solve? Why did it matter?

â¤ ğ—£ğ—¿ğ—¼ğ—½ğ—¼ğ˜€ğ—²ğ—± ğ—¦ğ—¼ğ—¹ğ˜‚ğ˜ğ—¶ğ—¼ğ—»: What did you build? How did it solve the problem?

â¤ ğ—¬ğ—¼ğ˜‚ğ—¿ ğ—¥ğ—¼ğ—¹ğ—²: What exactly did you do? Any challenges you handled?

â¤ ğ—§ğ—²ğ—°ğ—µ & ğ—§ğ—¼ğ—¼ğ—¹ğ˜€: Mention the tech stack. Keep it relevant.

â¤ ğ—œğ—ºğ—½ğ—®ğ—°ğ˜: What changed because of your work? Share outcomes
.
â¤ ğ—§ğ—²ğ—®ğ—º ğ—ªğ—¼ğ—¿ğ—¸: If it was a team project, how did you contribute?

â¤ ğ—Ÿğ—²ğ—®ğ—¿ğ—»ğ—¶ğ—»ğ—´: What did you learn? What would you do differently?

hashtag#Protip: Remember, ğ—°ğ—¼ğ—ºğ—ºğ˜‚ğ—»ğ—¶ğ—°ğ—®ğ˜ğ—¶ğ—¼ğ—» ğ—¶ğ˜€ ğ—¸ğ—²ğ˜†. And the best way to get better at it is to practice with mock interviews, get feedback from Senior Data Professionals and refine your answers until you can explain them confidently.


ğ—•ğ—®ğ˜€ğ—¶ğ—°ğ˜€ ğ—¼ğ—³ ğ—£ğ˜†ğ—¦ğ—½ğ—®ğ—¿ğ—¸:
- PySpark Architecture
- SparkContext and SparkSession
- RDDs (Resilient Distributed Datasets)
- DataFrames
- Transformations and Actions
- Lazy Evaluation

ğ—£ğ˜†ğ—¦ğ—½ğ—®ğ—¿ğ—¸ ğ——ğ—®ğ˜ğ—®ğ—™ğ—¿ğ—®ğ—ºğ—²ğ˜€:
- Creating DataFrames
- Reading Data from CSV, JSON, Parquet
- DataFrame Operations
- Filtering, Selecting, and Aggregating Data
- Joins and Merging DataFrames
- Working with Null Values

ğ—£ğ˜†ğ—¦ğ—½ğ—®ğ—¿ğ—¸ ğ—–ğ—¼ğ—¹ğ˜‚ğ—ºğ—» ğ—¢ğ—½ğ—²ğ—¿ğ—®ğ˜ğ—¶ğ—¼ğ—»ğ˜€:
- Defining and Using UDFs (User Defined Functions)
- Column Operations (Select, Rename, Drop)
- Handling Complex Data Types (Array, Map)
- Working with Dates and Timestamps

ğ—£ğ—®ğ—¿ğ˜ğ—¶ğ˜ğ—¶ğ—¼ğ—»ğ—¶ğ—»ğ—´ ğ—®ğ—»ğ—± ğ—¦ğ—µğ˜‚ğ—³ğ—³ğ—¹ğ—² ğ—¢ğ—½ğ—²ğ—¿ğ—®ğ˜ğ—¶ğ—¼ğ—»ğ˜€:
- Understanding Partitions
- Repartitioning and Coalescing
- Managing Shuffle Operations
- Optimizing Partition Sizes for Performance

ğ—–ğ—®ğ—°ğ—µğ—¶ğ—»ğ—´ ğ—®ğ—»ğ—± ğ—£ğ—²ğ—¿ğ˜€ğ—¶ğ˜€ğ˜ğ—¶ğ—»ğ—´ ğ——ğ—®ğ˜ğ—®:
- When to Cache or Persist
- Memory vs Disk Caching
- Checking Storage Levels

ğ—£ğ˜†ğ—¦ğ—½ğ—®ğ—¿ğ—¸ ğ—ªğ—¶ğ˜ğ—µ ğ—¦ğ—¤ğ—Ÿ:
- Spark SQL Introduction
- Creating Temp Views
- Running SQL Queries
- Optimizing SQL Queries with Catalyst Optimizer
- Working with Hive Tables in PySpark

ğ—ªğ—¼ğ—¿ğ—¸ğ—¶ğ—»ğ—´ ğ˜„ğ—¶ğ˜ğ—µ ğ——ğ—®ğ˜ğ—® ğ—¶ğ—» ğ—£ğ˜†ğ—¦ğ—½ğ—®ğ—¿ğ—¸:
- Data Cleaning and Preparation
- Handling Missing Values
- Data Normalization and Transformation
- Working with Categorical Data

ğ—”ğ—±ğ˜ƒğ—®ğ—»ğ—°ğ—²ğ—± ğ—§ğ—¼ğ—½ğ—¶ğ—°ğ˜€ ğ—¶ğ—» ğ—£ğ˜†ğ—¦ğ—½ğ—®ğ—¿ğ—¸:
- Broadcasting Variables
- Accumulators
- PySpark Window Functions
- PySpark with Machine Learning (MLlib)
- Working with Streaming Data (Spark Streaming)

ğ—£ğ—²ğ—¿ğ—³ğ—¼ğ—¿ğ—ºğ—®ğ—»ğ—°ğ—² ğ—§ğ˜‚ğ—»ğ—¶ğ—»ğ—´ ğ—¶ğ—» ğ—£ğ˜†ğ—¦ğ—½ğ—®ğ—¿ğ—¸:
- Understanding Job, Stage, and Task
- Tungsten Execution Engine
- Memory Management and Garbage Collection
- Tuning Parallelism
- Using Spark UI for Performance Monitoring
